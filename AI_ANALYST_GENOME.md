# AI ANALYST GENOME v1.0

> **What this is:** A self-contained bootstrap document. Paste this into an empty
> repo with Claude Code (Amazon SageMaker via Amazon Bedrock) and it will
> autonomously build a complete AI Data Analyst product — agents, skills,
> helpers, templates, and pipeline — through a 3-layer inception model.
>
> **What this is NOT:** A finished product. This is the seed. It creates the
> architects who create the builders who create the product.
>
> **Powered by [AI Analyst Lab](https://aianalystlab.ai)**
> Courses, consulting, and community for product teams building analytical independence.
> Created by Shane Butler, Sravya Madipalli, and Hai Guan.

---

## 0. Preamble

```yaml
genome_version: 1.0
min_claude_model: claude-sonnet-4-5-20250929
recommended_model: claude-opus-4-6
environment: amazon-sagemaker
llm_provider: amazon-bedrock
target_product: AI Data Analyst
estimated_build_time: "3-5 sessions"
origin: "AI Analyst Lab | aianalystlab.ai"
creators:
  - Shane Butler
  - Sravya Madipalli
  - Hai Guan
```

### How It Works

```
This Document (Genome)
  │
  ├─ Setup Wizard ──→ genome_config.yaml
  │
  ├─ Phase 0: Architecture Debate (4 agents, 4 rounds)
  │   └──→ _build/BUILD_PLAN.md + BUILD_STATUS.yaml
  │
  ├─ Phase 1: Build Execution (7 builders, 3 tiers)
  │   └──→ agents/ + .claude/skills/ + helpers/ + templates/
  │
  └─ Phase 2: The Product (19 agents + ~30 skills)
      └──→ Working AI Data Analyst
```

### Bootstrap Command

When this file is pasted, Claude should:
1. Read this entire document
2. Run the Setup Wizard (Section 1)
3. Save `genome_config.yaml` to the repo root
4. Execute Phase 0 (Section 2)
5. Execute Phase 1 (Section 3)
6. Report completion status

Say: **"I've read the AI Analyst Genome. Let's configure your AI Data Analyst. I have 7 questions."**

> **File naming:** This document should be saved as `AI_ANALYST_GENOME.md`.
> A companion `CLAUDE.md` bootstrapper auto-triggers Claude Code to read it.

---

## 1. Setup Wizard

Run this conversational flow. Do not present a form. Ask one question at a time.
Use the default if the user says "default" or just presses enter.

### Questions

**Q1: Data Platform**
> "Where does your data live? Tell me about your database or data warehouse,
> or if you're working with flat files."
> (Free-form — user describes their setup. Could be Snowflake, PostgreSQL,
> BigQuery, Redshift, Databricks, CSV/JSON files, or anything else.)
> → Maps to `config.data_platform` (normalized to a platform identifier)

Follow-up:
> "Do you already have connector scripts or libraries that talk to your data?"
> - YES → "Point me to them — I'll inspect and wrap them."
>   → Set `config.existing_connectors: true`, ask for path
> - NO → "I'll figure out what connection details I need based on your platform.
>   What credentials or access method do you use?"
>   → Collect whatever is relevant into `config.connection` block

**Q2: Dataset Description**
> "Describe your data in a sentence or two. What does it represent?"
> (Required — no default)
> → Maps to `config.dataset_description`

**Q3: Primary Use Case**
> "What's the main question this analyst should help answer?"
> (Required — no default)
> → Maps to `config.primary_use_case`

**Q4: Audience**
> "Who will see the outputs? (e.g., product team, executives, engineering)"
> Default: "Product team"
> → Maps to `config.audience`

**Q5: Capability Tier**
> "Which capability level?"
> - **Full** (default): 19 agents, ~30 skills, complete pipeline with root
>   cause investigation, opportunity sizing, experiment design, presentations
> - **Core**: ~12 agents, ~20 skills — analysis + charts, no experiment
>   design or opportunity sizing
> - **Minimal**: ~8 agents, ~12 skills — data exploration + basic analysis,
>   no presentation layer
> → Maps to `config.capability_tier`

**Q6: Brand**
> "Custom brand colors? Paste a JSON object with your tokens, or say 'default'
> for the AI Analyst Lab look."
> Default: AI Analyst Lab tokens (see Section 9)
> → Maps to `config.brand`
> Note: Default branding includes AI Analyst Lab attribution in deck footers
> and analysis outputs. Override brand to customize.

**Q7: Confirmation**
> Display a summary table of all answers. Ask: "Ready to build? (yes / adjust)"
> → On "yes", save `genome_config.yaml` and proceed to Phase 0

### Config Output Schema

Save to `genome_config.yaml`:

```yaml
# Auto-generated by AI Analyst Genome Setup Wizard
# Powered by AI Analyst Lab | aianalystlab.ai
project_name: "{{user_provided_or_derived}}"
data_platform: "{{normalized platform identifier}}"
dataset_description: "{{Q2 answer}}"
primary_use_case: "{{Q3 answer}}"
audience: "{{Q4 answer}}"
capability_tier: full | core | minimal

connection:
  # Platform-specific fields collected during Q1.
  # Phase 0 architects design the full connectivity layer based on this.
  # Examples: host, port, database, schema, warehouse, role, project_id, etc.
  details: {}
  credential_source: "{{how user authenticates — env vars, secrets manager, key file, etc.}}"
  credential_ref: ""

existing_connectors:
  enabled: false
  path: ""

attribution:
  company: "AI Analyst Lab"
  url: "https://aianalystlab.ai"
  creators: ["Shane Butler", "Sravya Madipalli", "Hai Guan"]
  tagline: "Built with AI Analyst Lab"
  footer_text: "Powered by AI Analyst Lab | aianalystlab.ai"
  show_attribution: true  # Set false to remove branding from outputs

brand:
  primary: "#1a1a2e"
  secondary: "#D97706"
  accent: "#D97706"
  accent_light: "#F0A060"
  bg_dark: "#1A1A17"
  bg_light: "#F7F6F2"
  chart_palette: ["#D97706", "#DC2626", "#059669", "#6B7280", "#4cc9f0", "#7209b7"]
  chart_bg: "#F7F6F2"
  font_heading: "Inter"
  font_body: "Inter"
  positive: "#059669"
  negative: "#DC2626"
```

---

## 2. Phase 0 — Architecture Debate

Phase 0 creates the build plan. Four agents work through a 4-round protocol:
independent drafts, cross-review, synthesis, and validation.

### 2.1 Phase 0 Agents

Read `genome_config.yaml` before starting. All agents receive the full config
as context.

**AGENT: Product Architect**
```
You are the Product Architect for an AI Data Analyst product.

YOUR JOB: Design the system shape — directory tree, agent boundaries, data
flow, and integration points.

CONTEXT: {{genome_config.yaml contents}}

PRODUCE:
1. Directory tree (every folder and key file)
2. Agent responsibility map (name → what it does → inputs → outputs)
3. Agent dependency DAG (which agents depend on which)
4. Integration point inventory (where agents hand off to each other)
5. Data flow diagram (how data moves from the user's platform → local cache
   → analysis → output). Design the connectivity layer based on the data
   platform specified in genome_config.yaml — do NOT hardcode any specific
   database technology. The connection architecture must be platform-agnostic
   with platform-specific adapters.

CONSTRAINTS:
- Directory structure must include: agents/, .claude/skills/, helpers/,
  templates/, .knowledge/datasets/, data/, working/, outputs/, _build/
- Agent count targets by tier: Full=19, Core=12, Minimal=8
- Every agent must have exactly one file in agents/
- The .claude/skills/ path is NON-NEGOTIABLE (Claude Code skill discovery)
- Pipeline agents have sequential dependencies. Standalone agents do not.
- helpers/ contains Python modules only — no agents, no skills
- working/ is for intermediate artifacts. outputs/ is for final deliverables.
- CLAUDE.md must stay under 350 lines

OUTPUT FORMAT:
Write to: _build/working/phase0_product_architect.md
```

**AGENT: Quality Systems Designer**
```
You are the Quality Systems Designer for an AI Data Analyst product.

YOUR JOB: Design validation, review, and testing strategy that ensures every
analysis is trustworthy.

CONTEXT: {{genome_config.yaml contents}}

PRODUCE:
1. 4-layer validation system spec:
   - Layer 1: Data Quality (nulls, duplicates, out-of-range) — pre-analysis
   - Layer 2: Statistical Rigor (appropriate tests, CIs, effect sizes) — during
   - Layer 3: Logical Coherence (business sense, contradictions) — post-analysis
   - Layer 4: Presentation Accuracy (charts match data, labels correct) — pre-output
2. Confidence scoring system (0-100, letter grades A-F, display as badge)
3. Review loop protocol (APPROVE / APPROVE WITH CHANGES / REJECT)
   - Max 2 revision cycles before escalation
   - Max 1 full rejection before user escalation
4. CONTRACT block schema (every agent must have one)
5. Registry validation rules (file existence, cycle detection, orphan check)
6. Acceptance criteria templates for each agent type

CONSTRAINTS:
- No heavy statistics: t-test, chi-square, CIs, and effect size are the ceiling
- No regression, no ML, no predictive modeling
- Simpson's Paradox check is mandatory before any conclusion
- Validation agent must run before any findings are presented
- Review loops apply within Phase 0 (debate) and Phase 1 (build)

OUTPUT FORMAT:
Write to: _build/working/phase0_quality_designer.md
```

**AGENT: DevEx Designer**
```
You are the DevEx Designer for an AI Data Analyst product.

YOUR JOB: Design the end-user experience — how someone interacts with this
AI Analyst day-to-day.

CONTEXT: {{genome_config.yaml contents}}

PRODUCE:
1. Skill naming conventions and trigger patterns
   - Slash-command skills (user invokes explicitly): /run-pipeline, /data, etc.
   - Auto-apply skills (system applies when conditions match): question-framing,
     triangulation, etc.
2. First-run onboarding flow (what happens when user opens repo for first time)
   - Must include AI Analyst Lab welcome: "This AI Data Analyst was built
     using the AI Analyst Genome by AI Analyst Lab (aianalystlab.ai). Created
     by Shane Butler, Sravya Madipalli, and Hai Guan."
   - Show after initial greeting, before data exploration
3. Quick-start paths:
   - Simple question → direct answer (no pipeline)
   - Guided analysis → full pipeline
   - Just a chart → straight to chart-maker
4. Error taxonomy and recovery messages (user-friendly, always offer next step)
5. Help system (/help, /datasets, /history)
6. Question Router classification (L1-L5 complexity levels)
   - L1: Simple lookup → direct query
   - L2: Comparison → query + chart
   - L3: Investigation → framing + analysis + validation
   - L4: Deep dive → full pipeline
   - L5: Strategic → full pipeline + experiment design

CONSTRAINTS:
- Skill files live in .claude/skills/{name}/skill.md — this is non-negotiable
- User should never need to know about agents — skills are the interface
- Error messages must always offer a path forward, never dead-end
- Adapt communication to detected user role (PM/DS/Eng vocabulary)

OUTPUT FORMAT:
Write to: _build/working/phase0_devex_designer.md
```

**AGENT: Build Arbiter**
```
You are the Build Arbiter. You do NOT produce an independent draft.

YOUR JOB: In Round 3, read the three proposals from Product Architect,
Quality Systems Designer, and DevEx Designer. Resolve every conflict and
produce the canonical BUILD_PLAN.

CONFLICT RESOLUTION PRINCIPLES (apply in order):
1. Dependency order wins — if A must exist before B works, A's proposal wins
2. Fewer integration points wins — simpler interfaces beat complex ones
3. Existing patterns win — proven architecture beats novel design
4. User-facing impact breaks ties — what the end-user sees matters most

For every conflict, log:
- Conflict ID (C1, C2, ...)
- What the conflict is
- Which proposals disagree
- Resolution and rationale

OUTPUT FORMAT:
- _build/BUILD_PLAN.md — complete task-level plan with file paths, types,
  acceptance criteria, and dependency ordering
- _build/BUILD_STATUS.yaml — all tasks pre-populated as not_started
- _build/DECISION_LOG.md — every conflict and its resolution
```

### 2.2 Debate Protocol

```
ROUND 1: INDEPENDENT DRAFTS (parallel)
  Product Architect     ──┐
  Quality Designer      ──┤  Each produces independent proposal
  DevEx Designer        ──┘
  Write to: _build/working/phase0_{role}.md

ROUND 2: CROSS-REVIEW (parallel)
  Each agent reads the other two proposals.
  For each section of each proposal, annotate: AGREE / DISAGREE / EXTEND
  Write annotations to: _build/working/phase0_{role}_review.md
  Compile conflict registry: _build/working/phase0_conflicts.md

ROUND 3: SYNTHESIS (Build Arbiter)
  Read all 3 drafts + all annotations + conflict registry.
  Resolve every conflict using the 4 principles above.
  Produce: BUILD_PLAN.md, BUILD_STATUS.yaml, DECISION_LOG.md

ROUND 4: VALIDATION (parallel)
  All 3 original agents review the Arbiter's plan.
  Can raise BLOCKING objections only — must cite a specific failure mode.
  Arbiter addresses or explains each objection.
  Emit: Final BUILD_PLAN.md (updated if needed)
```

### 2.3 Build Plan Output Format

`_build/BUILD_PLAN.md` must contain:

```markdown
# Build Plan
## Project: {{config.project_name}}
## Generated: {{date}}
## Capability Tier: {{config.capability_tier}}

### Directory Structure
(exact tree with every folder and file to be created)

### Wave Model
Wave 0: Foundation (directories, config, data connection, brand tokens)
Wave 1: Core Infrastructure (helpers, base templates, knowledge structure)
Wave 2: Pipeline Agents (all analysis agents in DAG order)
Wave 3: Skills & UX (all skills, CLAUDE.md, onboarding)
Wave 4: Integration & Review (registry, validation, end-to-end test)

### Task Registry
(every task with: id, wave, description, file_path, file_type, depends_on,
 acceptance_criteria, estimated_scope)
```

`_build/BUILD_STATUS.yaml` schema:

```yaml
project: "{{config.project_name}}"
genome_version: "1.0"
master_plan: "_build/BUILD_PLAN.md"
started: "{{date}}"
last_updated: "{{date}}"
capability_tier: "{{config.capability_tier}}"
session_log: []
tasks: []
  # Each task:
  # - id: "W0-1.1"        (Wave-Section.Item)
  #   wave: 0
  #   description: ""
  #   status: not_started | in_progress | completed | failed | blocked
  #   depends_on: []
  #   output_files: []
  #   session: null
  #   acceptance_criteria: ""
  #   notes: ""
```

---

## 3. Phase 1 — Build Execution

Phase 1 builders execute the BUILD_PLAN produced by Phase 0. They create the
actual files that constitute the product.

### 3.1 Builder Definitions (7 builders)

Each builder reads `_build/BUILD_PLAN.md` and `_build/BUILD_STATUS.yaml` to
determine what to create. After completing each task, update BUILD_STATUS.yaml.

**Build DAG (3 tiers):**

```
Tier 1 (parallel):
  skill-author ──────────────┐
  agent-spec-author ─────────┤
  helper-author ─────────────┤──→ Tier 2 ──→ Tier 3
  template-author ───────────┘
                                    │              │
Tier 2 (parallel):                  │              │
  registry-assembler ───────────────┘              │
  integration-reviewer ────────────────────────────┘

Tier 3 (sequential):
  claude-md-assembler ─────────────────────────────→ DONE
```

**BUILDER: skill-author** (Tier 1)
```
Read the BUILD_PLAN skill inventory. For each skill, create:
  .claude/skills/{name}/skill.md

Each skill file must contain:
- Purpose (1-2 sentences)
- When to Use (trigger condition)
- Instructions (step-by-step)
- Output format
- A CONTRACT block (see Section 7.2)

Skill categories:
- Data Platform: connect-data, switch-dataset, datasets, data-inspect,
  data-profiling, knowledge-bootstrap, data-quality-check, cache-sync
- Analytical Guardrails: question-framing, metric-spec, tracking-gaps,
  triangulation, guardrails, simpsons-paradox, analysis-design-spec,
  close-the-loop
- Pipeline/UX: run-pipeline, resume-pipeline, question-router,
  first-run-welcome, explore, export, forecast, history, patterns,
  semantic-validation, archive-analysis
- Presentation: visualization-patterns, presentation-themes,
  stakeholder-communication

Acceptance: File exists, has PURPOSE + TRIGGER + INSTRUCTIONS + CONTRACT.
```

**BUILDER: agent-spec-author** (Tier 1)
```
Read the BUILD_PLAN agent architecture. For each agent, create:
  agents/{name}.md

Each agent file must contain:
- Identity block (who you are, what you do)
- Input variables with {{VARIABLE}} placeholders
- Step-by-step workflow
- Output specification (file paths, format)
- Quality checks (what to validate before emitting output)
- A CONTRACT block (see Section 7.2)

Pipeline agents (17 — see Section 4.1 for full list):
  question-framing, hypothesis, data-explorer, source-tieout,
  descriptive-analytics, overtime-trend, cohort-analysis,
  root-cause-investigator, validation, opportunity-sizer,
  story-architect, narrative-coherence-reviewer, chart-maker,
  visual-design-critic, storytelling, deck-creator, close-the-loop

Standalone agents (2):
  experiment-designer, connector-inspector

Acceptance: File exists, has IDENTITY + INPUTS + WORKFLOW + OUTPUTS + CONTRACT.
Every {{VARIABLE}} is documented. No hardcoded dataset names.
```

**BUILDER: helper-author** (Tier 1)
```
Read the BUILD_PLAN helper specification. Create Python modules in helpers/:

Required modules:
- chart_helpers.py — matplotlib wrappers, brand-token styling, SWD patterns
    Key functions: swd_style(), highlight_bar(), highlight_line(),
    action_title(), annotate_point(), save_chart(), stacked_bar(),
    retention_heatmap(), funnel_waterfall(), sensitivity_table()
- sql_helpers.py — SQL generation, dialect adapters, sanity checks
    Key functions: get_dialect(), check_join_cardinality(),
    check_percentages_sum(), check_date_bounds(), check_no_duplicates()
- data_helpers.py — DataFrame profiling, data quality, source detection
    Key functions: detect_active_source(), check_connection(),
    get_local_connection(), read_table(), list_tables(), schema_to_markdown()
- stats_helpers.py — Statistical tests (NO regression, NO ML)
    Key functions: two_sample_proportion_test(), two_sample_mean_test(),
    confidence_interval(), chi_squared_test(), bootstrap_ci(),
    format_significance(), interpret_effect_size()
- cache_helpers.py — Query result caching and fallback to local/static files
    Key functions: cache_query(), get_cached(), invalidate(), fallback_read()
    The caching implementation must be determined by Phase 0 architects based
    on the user's data platform. Do NOT hardcode any specific cache technology.
- error_helpers.py — User-friendly error messages
    Key functions: friendly_error(), safe_query(), check_empty_dataframe(),
    suggest_column()

chart_helpers.py MUST read brand tokens from genome_config.yaml (or the
brand section) to set colors. No hardcoded color values in chart code.
Charts MUST include attribution watermark from config.attribution when
show_attribution is true — small text in bottom-right: "AI Analyst Lab".

sql_helpers.py MUST support dialect switching based on the user's platform.
Use get_dialect(connection_type) → returns SQL templates for date_trunc,
safe_divide, etc. Phase 0 architects determine which dialects to support
based on the configured data platform. Do NOT hardcode a fixed list.

Acceptance: Each file imports cleanly. Functions have docstrings. No
hardcoded dataset names, schema names, credentials, or database technology
assumptions.
```

**BUILDER: template-author** (Tier 1)
```
Create presentation templates and themes:

templates/deck_skeleton.marp.md
  - Marp frontmatter (marp, theme, size, paginate, html, footer)
  - Footer MUST include: "{{config.attribution.footer_text}}" when
    show_attribution is true (default: "Powered by AI Analyst Lab | aianalystlab.ai")
  - Title slide template — include attribution tagline on title slides
  - Section opener template
  - KPI row template
  - Chart slide templates (chart-full, chart-left, chart-right)
  - Insight slide template
  - Recommendation slide template
  - Appendix slide template

templates/marp_components.md
  - HTML snippet library for all custom components
  - .kpi-row, .so-what, .finding, .rec-row, .chart-container

themes/analytics.css (light theme)
themes/analytics-dark.css (dark theme)
  - Both generated from brand tokens in genome_config.yaml
  - Marp-compatible CSS with custom slide classes

helpers/analytics_chart_style.mplstyle
  - Matplotlib style: chart_bg background, no top/right spines, no grid,
    sans-serif fonts, 150 DPI

Acceptance: Marp can render the skeleton. CSS has all slide classes. Colors
match genome_config brand tokens exactly.
```

**BUILDER: registry-assembler** (Tier 2)
```
After all Tier 1 builders complete, assemble:
  agents/registry.yaml

Read every agents/*.md file. Extract the CONTRACT block from each.
Build the machine-readable DAG:

version: 1
agents:
  - name: (from CONTRACT)
    file: agents/{name}.md
    pipeline_step: (from CONTRACT)
    depends_on: (from CONTRACT)
    inputs: (from CONTRACT)
    outputs: (from CONTRACT)
    knowledge_context: (from CONTRACT)

VALIDATION (must all pass):
1. Every agent file referenced exists on disk
2. Every name in depends_on references an agent in this file
3. No cycles (topological sort via Kahn's algorithm must succeed)
4. No orphan pipeline agents (every pipeline agent reachable from a root)
5. Standalone agents have pipeline_step: null
6. Every review_by reference is valid

If validation fails, report which rule failed and halt.

Acceptance: registry.yaml parses. All 5 validation rules pass.
```

**BUILDER: integration-reviewer** (Tier 2)
```
After Tier 1 completes, verify cross-cutting integration:

CHECK LIST:
1. All agent IDs in registry.yaml have spec files in agents/
2. All skills referenced in BUILD_PLAN exist in .claude/skills/
3. All Python imports in helpers/ resolve (no missing dependencies)
4. All brand tokens in genome_config match CSS themes and chart_helpers
5. DAG is acyclic, no orphans (confirm registry-assembler's work)
6. CLAUDE.md line count ≤ 350
7. No hardcoded dataset names anywhere (grep for any dataset-specific strings)
8. Every agent's {{VARIABLE}} placeholders are documented
9. data_sources.yaml schema matches the data platform from config
10. .knowledge/ directory structure matches the Data Brain spec
11. AI Analyst Lab attribution present in: CLAUDE.md header, deck skeleton
    footer, chart_helpers watermark function, first-run-welcome, /help output
12. No hardcoded database technology names in agents or skills (grep check)

Write report to: _build/integration_review.md
Status: PASS / FAIL (with specific failures listed)

If FAIL: List exact files and line numbers. Tier 1 builders fix their files.
Re-run integration review. Max 2 cycles.

Acceptance: Integration review status is PASS.
```

**BUILDER: claude-md-assembler** (Tier 3)
```
After Tier 2 completes (registry + integration review both pass), assemble:
  CLAUDE.md

This is the persona file. It turns Claude Code from a general-purpose
assistant into an AI Data Analyst. MAX 350 LINES.

Structure:
1. Who You Are — identity, style, personality
2. Quick Start — 5 common entry points
3. What You Do — capabilities list (what you do / don't do)
4. Your Skills — table of all skills with paths and triggers
5. Your Agents — table of all agents with paths and invocation conditions
6. System Variables — {{DATE}}, {{DATASET_NAME}}, etc.
7. Default Workflow — numbered steps through the analysis process
8. Available Data — how to load and switch datasets
9. Rules — non-negotiable quality rules
10. When Things Go Wrong — error recovery table

CRITICAL RULES FOR CLAUDE.MD:
- File header comment: "Built with the AI Analyst Lab AI Analyst Genome (aianalystlab.ai)"
- Skills table: every .claude/skills/*/skill.md must appear
- Agents table: every agents/*.md must appear
- Default workflow must reference agents and skills by name
- Must include dataset isolation rule: never hardcode table/schema/column names
- Must include data source fallback cascade
- Must include 4-layer validation requirement
- Must stay under 350 lines — extract to INDEX files if needed
- No hardcoded database technology anywhere — resolve from config

Acceptance: CLAUDE.md exists, ≤350 lines, references every agent and skill,
contains all 10 sections, includes AI Analyst Lab attribution in header.
```

### 3.2 Session Protocol

Each build session follows this protocol:

```
SESSION START:
1. Read _build/BUILD_STATUS.yaml
2. Compute READY set: tasks whose dependencies are all completed
3. Announce: "Session N starting. X tasks ready, Y completed, Z total."

SESSION EXECUTE:
4. Run builders in DAG tier order
5. After each task completes:
   - Update BUILD_STATUS.yaml (status → completed, session → N)
   - If task fails: mark failed, block dependents, continue independents

SESSION END:
6. Update BUILD_STATUS.yaml with session log entry
7. Display summary: completed / failed / remaining
8. If failures exist: list them with suggested fixes
```

### 3.3 Quality Gates

Between tiers, run quality gates:

| Gate | Between | Checks |
|------|---------|--------|
| G1 | Tier 1 → Tier 2 | All Tier 1 files exist, CONTRACT blocks parse, no syntax errors in Python |
| G2 | Tier 2 → Tier 3 | registry.yaml valid, integration review PASS |
| G3 | After Tier 3 | CLAUDE.md valid, end-to-end dry run of /run-pipeline succeeds |

---

## 4. Phase 2 — The Product Specification

This section defines what the Phase 1 builders must create. It is the
reference architecture for the AI Data Analyst.

### 4.1 Agent Architecture

**Pipeline Agents (17) — execute in DAG order:**

| # | Agent | DAG Step | Purpose | Key Inputs | Key Outputs |
|---|-------|----------|---------|------------|-------------|
| 1 | question-framing | 1 | Apply Question Ladder: goal → decision → metric → hypothesis | BUSINESS_CONTEXT | question_brief.md |
| 2 | hypothesis | 3 | Generate testable hypotheses across 4 categories | QUESTION_BRIEF | hypothesis_doc.md |
| 3 | data-explorer | 4 | Discover dataset contents, quality, gaps | DATA_SOURCE | data_inventory.md |
| 4 | source-tieout | 4.5 | Verify data loading integrity (dual-path comparison) | DATA_SOURCE, TABLE_MAPPING | tieout_report.md |
| 5 | descriptive-analytics | 5 | Segmentation, funnels, drivers analysis | DATASET, QUESTION_BRIEF | analysis_report.md, charts/ |
| 6 | overtime-trend | 5 | Time-series patterns, anomalies, seasonality | DATASET, TIME_COLUMN | trend_report.md, charts/ |
| 7 | cohort-analysis | 5 | Retention curves, LTV, vintage comparison | COHORT_DIMENSION, DATASET | cohort_report.md, charts/ |
| 8 | root-cause-investigator | 6 | Iterative drill-down using 8-step protocol | METRIC, OBSERVATION, DATASET | investigation.md |
| 9 | validation | 7 | 4-layer validation, confidence scoring | ANALYSIS_CODE, RESULTS | validation_report.md |
| 10 | opportunity-sizer | 8 | Business impact quantification with sensitivity | OPPORTUNITY, RESULTS | sizing_report.md |
| 11 | story-architect | 9 | Design narrative arc (Context-Tension-Resolution) | RESULTS, QUESTION_BRIEF | storyboard.md |
| 12 | narrative-coherence-reviewer | 10 | Review story flow before charting | STORYBOARD | coherence_review.md |
| 13 | chart-maker | 12 | Generate visualizations per storyboard specs | DATA, CHART_SPEC, THEME | charts/*.png |
| 14 | visual-design-critic | 13 | Review charts against SWD checklist | CHART_FILES, STORYBOARD | design_review.md |
| 15 | storytelling | 15 | Write prose narrative from storyboard | RESULTS, STORYBOARD, AUDIENCE | narrative.md |
| 16 | deck-creator | 16 | Assemble Marp slide deck | NARRATIVE, CHARTS, THEME | deck.marp.md |
| 17 | close-the-loop | 18 | Ensure recommendations have owners, metrics, dates | RESULTS, RECOMMENDATIONS | close_the_loop.md |

**Standalone Agents (2):**

| Agent | Purpose | Invoked When |
|-------|---------|-------------|
| experiment-designer | A/B test briefs with power estimation and decision rules | User wants to test a causal hypothesis |
| connector-inspector | Inspect user's existing data connectors, generate adapter | Setup wizard "I have connectors" path |

### 4.2 Skill Inventory

**Data Platform (8):**
| Skill | Trigger | Purpose |
|-------|---------|---------|
| connect-data | `/connect-data` | Add a new dataset connection |
| switch-dataset | `/switch-dataset {name}` | Change active dataset |
| datasets | `/datasets` | List all connected datasets |
| data-inspect | `/data` or `/data {table}` | Show active schema |
| data-profiling | After connecting new dataset | Deep-profile schema, distributions, anomalies |
| knowledge-bootstrap | Session start | Load active dataset context |
| data-quality-check | Starting any analysis | Pre-analysis data quality scan |
| cache-sync | `/cache-sync` | Sync remote data platform → local cache |

**Analytical Guardrails (8):**
| Skill | Trigger | Purpose |
|-------|---------|---------|
| question-framing | Receiving a vague business question | Apply Question Ladder framework |
| metric-spec | Defining or documenting a metric | Metric specification template |
| tracking-gaps | Analysis requires possibly-missing data | Identify tracking gaps |
| triangulation | After findings, before presenting | Cross-validate from multiple angles |
| guardrails | Defining metrics or reporting positive findings | Check for trade-offs and guardrail metrics |
| simpsons-paradox | Before any conclusion | Segment-first aggregation paradox check |
| analysis-design-spec | Starting any new analysis | Confirm question, decision, data, dimensions |
| close-the-loop | End of any analysis with recommendations | Ensure follow-up tracking |

**Pipeline / UX (11):**
| Skill | Trigger | Purpose |
|-------|---------|---------|
| run-pipeline | `/run-pipeline` | End-to-end analysis, DAG-based execution |
| resume-pipeline | `/resume-pipeline` | Resume from last completed step |
| question-router | Every analytical request | Classify L1-L5, route appropriately |
| first-run-welcome | First session, no user profile | Onboarding flow — includes AI Analyst Lab intro and creator credits |
| explore | `/explore` | Quick interactive data exploration |
| export | `/export {format}` | Export as slides, email, brief, data |
| forecast | Time-series projection request | Simple time-series forecasting |
| history | `/history` | View past analyses |
| patterns | Detecting recurring patterns | Cross-analysis pattern detection |
| semantic-validation | After validation agent | Semantic cross-checks |
| archive-analysis | End of pipeline | Archive results to .knowledge/ |

**Presentation (3):**
| Skill | Trigger | Purpose |
|-------|---------|---------|
| visualization-patterns | Generating any chart | SWD chart patterns and rules |
| presentation-themes | Creating a deck | Theme selection and rules |
| stakeholder-communication | Producing narrative or deck | Audience-adapted communication |

### 4.3 Pipeline DAG

```
question-framing (1)──→ hypothesis (3)
                    └──→ data-explorer (4) ──→ source-tieout (4.5) ──┐
                                                                      ├──→ descriptive-analytics (5)
                                                                      ├──→ overtime-trend (5)
                                                                      └──→ cohort-analysis (5)
                                                                               │
descriptive-analytics ──→ root-cause-investigator (6)                          │
                                │                                              │
                          validation (7) ←─────────────────────────────────────┘
                                │
                          opportunity-sizer (8)
                                │
                          story-architect (9) ──→ narrative-coherence-reviewer (10)
                                                        │
                                                  chart-maker (12)
                                                        │
                                                  visual-design-critic (13)
                                                        │
                                                  storytelling (15)
                                                        │
                                                  deck-creator (16) ──→ close-the-loop (18)
```

### 4.4 Helper Modules

See Section 3.1 (helper-author) for the complete specification. Summary:

| Module | Purpose | Key Dependencies |
|--------|---------|-----------------|
| chart_helpers.py | Charting library + SWD patterns + attribution watermark | matplotlib, numpy |
| sql_helpers.py | SQL generation + dialect adapters | (stdlib only) |
| data_helpers.py | Data source access + profiling | pandas + platform connector |
| stats_helpers.py | Statistical tests (no ML) | scipy, numpy |
| cache_helpers.py | Query cache + fallback | pandas + caching layer (Phase 0 decides) |
| error_helpers.py | User-friendly errors | (stdlib only) |

### 4.5 Templates & Themes

| File | Purpose |
|------|---------|
| templates/deck_skeleton.marp.md | Marp slide deck skeleton with frontmatter |
| templates/marp_components.md | HTML snippet library (.kpi-row, .so-what, etc.) |
| themes/analytics.css | Light theme (chart_bg background) |
| themes/analytics-dark.css | Dark theme (bg_dark background) |
| helpers/analytics_chart_style.mplstyle | Matplotlib style file |

---

## 5. Data Platform Layer

The data platform is **entirely determined by the Setup Wizard** and designed
by the **Phase 0 Product Architect**. This section defines the architecture
pattern — not the specific technology.

### 5.1 Connection Architecture Pattern

```
Primary: User's data platform (live queries)
    ↓ (cache — technology chosen by Phase 0 based on platform)
Secondary: Local cache (query results stored locally)
    ↓ (fallback)
Tertiary: CSV/Parquet (static file fallback)
```

At analysis start, verify connectivity:
1. Read `.knowledge/datasets/{active}/manifest.yaml`
2. Try primary connection — run a simple health check query
3. If fails → try local cache
4. If fails → use static file fallback if available
5. Always inform user which source is active

### 5.2 Credential Management

Resolution order (SageMaker/Bedrock environment):

1. **Environment variables** — platform-specific env vars
2. **AWS Secrets Manager** — secret name from `data_sources.yaml` → `credential_ref`
3. **Key-based authentication** — key file or token path
4. **Interactive prompt** — last resort, ask the user

Never store credentials in code, config files, or git. Phase 0 architects
design the specific credential flow based on the user's platform and
authentication method.

### 5.3 data_sources.yaml

```yaml
# data_sources.yaml — Connection registry
# HOW to connect. WHAT it contains lives in .knowledge/datasets/
# Active dataset pointer: .knowledge/active.yaml

sources:
  {{dataset_name}}:
    display_name: "{{config.dataset_description | truncate(50)}}"
    connection_type: "{{config.data_platform}}"
    connection_details: {{config.connection.details}}  # Platform-specific
    credential_source: "{{config.connection.credential_source}}"
    credential_ref: "{{config.connection.credential_ref}}"
    fallback:
      type: "{{local cache type — decided by Phase 0}}"
      path: ".cache/{{dataset_name}}.{{ext}}"
    fallback_available: false
    registered_at: "{{date}}"
```

### 5.4 Cache & Fallback

- First query to remote platform → cache result locally
- Subsequent identical queries → serve from local cache
- Cache invalidation: manual via `/cache-sync` or TTL-based (configurable)
- If remote platform unreachable → serve from local cache
- If local cache empty → fall back to CSV/Parquet if available
- Always tell the user which source is active
- Phase 0 architects decide the specific caching technology based on the
  user's data platform and query patterns

### 5.5 Data Brain (.knowledge/)

```
.knowledge/
  active.yaml                          # Points to current dataset
  datasets/
    {{dataset_name}}/
      manifest.yaml                    # Connection, summary stats
      schema.md                        # Table/column documentation
      quirks.md                        # Known data gotchas
      last_profile.md                  # Deep profiling results
      access_notes.md                  # Role permissions, PII flags
      metrics/                         # Registered metric definitions
        {{metric_name}}.yaml
  analyses/                            # Archived analysis results
  user/
    profile.yaml                       # Detected user role, preferences
  global/
    frameworks.md                      # Analytical framework reference
```

The `knowledge-bootstrap` skill loads this context at session start.
The `data-profiling` skill populates it when a new dataset is connected.
The `archive-analysis` skill saves completed analyses here.

---

## 6. Analytical Capability Layer

### 6.1 Capability Matrix

| Analysis Type | Agent | Key Skills | Tier Required |
|--------------|-------|------------|---------------|
| Funnel Analysis | descriptive-analytics (focus=funnel) | triangulation, guardrails | Core |
| Segmentation | descriptive-analytics (focus=segment) | simpsons-paradox | Core |
| Drivers Analysis | descriptive-analytics (focus=drivers) | triangulation | Core |
| Root Cause | root-cause-investigator | data-quality-check, triangulation | Full |
| Trend Analysis | overtime-trend | forecast | Core |
| Cohort Analysis | cohort-analysis | triangulation | Full |
| Metric Definition | (skill-driven) | metric-spec, guardrails | Core |
| Data Quality | data-explorer + validation | data-profiling | Core |
| Opportunity Sizing | opportunity-sizer | close-the-loop | Full |
| Experiment Design | experiment-designer | metric-spec | Full |

### 6.2 Key Frameworks

**The Question Ladder:**
Goal → Decision → Metric → Hypothesis → Analysis

**Hypothesis Categories (always check all 4):**
1. Product Changes — feature launches, UX modifications
2. Technical Issues — bugs, performance regressions, data pipeline breaks
3. External Factors — seasonality, market events, competitor moves
4. Mix Shift — composition changes masking or amplifying trends

**Root Cause Protocol (8 steps):**
1. **Confirm** — Is the observation real or a data artifact?
2. **Baseline** — What does "normal" look like? (historical range, peer comparison)
3. **Decompose** — Test every available dimension for explanatory power
4. **Isolate** — Which segment(s) explain the majority of variance?
5. **Narrow and Repeat** — Filter to isolated segment, remove used dimension, go to step 3
6. **Hypothesize** — Generate hypotheses across all 4 categories above
7. **Quantify** — Size the impact of the root cause
8. **Report** — Specific, actionable finding (not "payments increased" but "iOS v2.3 payment SDK regression affecting 23% of checkout attempts")

**Minimum depth gate:** Must reach decomposition Level 3 before evaluating
whether to terminate. Shallow root causes are almost always wrong.

### 6.3 Validation System (4 layers)

| Layer | Name | When | Checks |
|-------|------|------|--------|
| 1 | Data Quality | Pre-analysis | Nulls, duplicates, out-of-range, date bounds, cardinality |
| 2 | Statistical Rigor | During analysis | Appropriate tests, CIs, effect sizes, sample sizes |
| 3 | Logical Coherence | Post-analysis | Business plausibility, internal consistency, Simpson's check |
| 4 | Presentation Accuracy | Pre-output | Charts match data, labels correct, no rounding errors |

All 4 layers must pass before findings are presented. Failure at any layer
produces a BLOCKER that halts the pipeline.

### 6.4 Confidence Scoring

Every finding gets a confidence score:

| Grade | Score | Criteria |
|-------|-------|----------|
| A | 0.80-1.00 | Statistically significant, triangulated from multiple angles, no confounders identified |
| B | 0.65-0.79 | Significant, multiple supporting signals, minor caveats |
| C | 0.50-0.64 | Significant but limited sample or single data source |
| D | 0.35-0.49 | Suggestive pattern, not statistically significant |
| F | 0.00-0.34 | Hypothesis only, insufficient data to evaluate |

Display as a badge in executive summaries. Order recommendations by
confidence (A → F), never alphabetically.

### 6.5 Statistical Boundaries

**Permitted:** t-test, chi-square, confidence intervals, effect size
(Cohen's d, Cramer's V), bootstrap CI, proportion tests, Mann-Whitney U

**NOT permitted:** Regression, ML models, predictive modeling, clustering
algorithms, neural networks, time-series forecasting models (ARIMA, Prophet)

**Forecasting approach:** Simple linear extrapolation with confidence bands,
clearly labeled as projection, not prediction. Always caveat with
"assuming current trends continue."

---

## 7. Quality System

### 7.1 Review Loop Protocol

Applies within Phase 0 (debate), Phase 1 (build), and Phase 2 (runtime).

```
Producer creates artifact
    ↓
Reviewer evaluates
    ↓
APPROVE → proceed to next step
APPROVE WITH CHANGES → producer revises, reviewer confirms (max 2 cycles)
REJECT → producer starts fresh (max 1 rejection before user escalation)
```

If max cycles exceeded, escalate to user with:
- What was produced
- What the reviewer objected to
- Suggested resolution

### 7.2 CONTRACT Block Schema

Every agent and skill MUST include a CONTRACT block as an HTML comment:

```yaml
<!-- CONTRACT_START
name: agent-name
version: 1
layer: 0 | 1 | 2
pipeline_step: N | null
inputs:
  - name: INPUT_NAME
    type: str | file | query_result
    source: user | system | agent:upstream-name
    required: true | false
outputs:
  - path: working/output.md | outputs/output.md
    type: markdown | csv | json | image | yaml
depends_on: [upstream-agent-names]
review_by: reviewer-agent-name | null
max_tokens: 15000
CONTRACT_END -->
```

The registry-assembler reads these to build `agents/registry.yaml`.

### 7.3 Registry Validation Rules

Run at pre-flight (before any pipeline execution):

1. **File existence:** Every `agent.file` in registry must exist on disk
2. **Dependency resolution:** Every name in `depends_on` must be in the registry
3. **Cycle detection:** Topological sort (Kahn's algorithm) must succeed
4. **Orphan detection:** Every pipeline agent reachable from at least one root
5. **Contract compatibility:** Output types must match downstream input types
6. **Review chain validity:** Every `review_by` references a valid agent

### 7.4 Drift Prevention

Over time, agent quality can degrade. Prevent this:

- **Acceptance criteria:** Every task in BUILD_STATUS has explicit pass/fail criteria
- **Round-trip validation:** After a builder creates a file, a reviewer agent
  reads it back and checks it against the BUILD_PLAN specification
- **No hardcoded data:** Grep the entire repo for dataset-specific strings
  after every build wave. Any hardcoded reference is a build failure.
- **CONTRACT enforcement:** If an agent's actual outputs don't match its
  CONTRACT outputs block, that's a build failure

---

## 8. Build Tracking

### 8.1 Session Continuity

The build may span multiple Claude Code sessions. BUILD_STATUS.yaml provides
continuity.

**Session start protocol:**
```
1. Read _build/BUILD_STATUS.yaml
2. Read _build/BUILD_PLAN.md
3. Compute READY set (status=not_started, all depends_on=completed)
4. Compute BLOCKED set (depends_on includes a failed task)
5. Announce status: "Session N. Ready: X, Completed: Y, Failed: Z, Total: T"
```

**Session end protocol:**
```
1. Update BUILD_STATUS.yaml:
   - Add session_log entry
   - Update task statuses
   - Set last_updated date
2. Display summary table
3. If failures: list with suggested next steps
4. If all complete: "Build complete. Run /run-pipeline to test."
```

### 8.2 Wave Model

Tasks are organized into waves. Complete one wave before starting the next.

| Wave | Name | Contents |
|------|------|----------|
| 0 | Foundation | Directories, genome_config.yaml, data_sources.yaml, .knowledge/ structure |
| 1 | Core Infrastructure | helpers/*.py, base templates, themes, chart style |
| 2 | Pipeline Agents | All 17 pipeline agents + 2 standalone agents |
| 3 | Skills & UX | All ~30 skills, CLAUDE.md, first-run-welcome |
| 4 | Integration | registry.yaml, integration review, end-to-end validation |

### 8.3 Error Recovery

- **Task failure:** Mark as `failed`, set dependents to `blocked`, continue
  independent tasks. Log the error in task notes.
- **Wave failure:** If >30% of a wave's tasks fail, pause and report to user
  before continuing.
- **Build restart:** If BUILD_STATUS.yaml exists with completed tasks, resume
  from the READY set — never re-do completed work.
- **Dependency-scoped failure:** Only block direct dependents, not the entire
  downstream tree. If A→B→C and A fails, B is blocked but C might have
  alternate paths.

---

## 9. Brand & Attribution System

### 9.1 AI Analyst Lab Identity

This product is created by **AI Analyst Lab** (aianalystlab.ai).

```yaml
company: "AI Analyst Lab"
url: "https://aianalystlab.ai"
tagline: "Courses, consulting, and community for product teams building analytical independence."
creators:
  - name: Shane Butler
    role: Founder
  - name: Sravya Madipalli
    role: Instructor
  - name: Hai Guan
    role: Instructor
```

### 9.2 Attribution Placement

AI Analyst Lab attribution appears in these locations by default. Users can
set `config.attribution.show_attribution: false` to suppress, but it requires
an explicit edit to `genome_config.yaml`.

| Surface | Attribution Format | Where |
|---------|-------------------|-------|
| Slide deck footer | "Powered by AI Analyst Lab \| aianalystlab.ai" | Every slide via Marp footer |
| Slide deck title slide | "Built with AI Analyst Lab" | Subtitle area |
| Chart watermark | "AI Analyst Lab" | Small text, bottom-right of every chart |
| Analysis report header | "Analysis by {{project_name}} \| Powered by AI Analyst Lab" | Top of every output .md file |
| First-run welcome | Full company intro + creator names | Onboarding flow |
| /help output | "This AI Data Analyst was built using the AI Analyst Genome by AI Analyst Lab (aianalystlab.ai). Created by Shane Butler, Sravya Madipalli, and Hai Guan." | Help text footer |
| CLAUDE.md | "Built with the AI Analyst Lab AI Analyst Genome" | Comment in file header |
| _build/BUILD_PLAN.md | "Generated by AI Analyst Genome v1.0 — AI Analyst Lab" | File header |

### 9.3 Default Brand Tokens

If the user says "default" at Q6, use these AI Analyst Lab tokens:

```json
{
  "primary": "#1a1a2e",
  "secondary": "#D97706",
  "accent": "#D97706",
  "accent_light": "#F0A060",
  "bg_dark": "#1A1A17",
  "bg_light": "#F7F6F2",
  "chart_bg": "#F7F6F2",
  "chart_palette": ["#D97706", "#DC2626", "#059669", "#6B7280", "#4cc9f0", "#7209b7"],
  "font_heading": "Inter",
  "font_body": "Inter",
  "positive": "#059669",
  "negative": "#DC2626",
  "text_dark_primary": "#F5F5F0",
  "text_dark_secondary": "#A8A090",
  "text_light_primary": "#1F2937",
  "text_light_secondary": "#4B5563",
  "border_dark": "#363532",
  "border_light": "#E5E7EB"
}
```

### 9.4 Token Propagation

Brand tokens flow to exactly 3 places. No other source of truth for colors:

1. **themes/*.css** — Marp slide themes read from tokens
2. **helpers/chart_helpers.py** — `swd_style()` reads from tokens
3. **helpers/analytics_chart_style.mplstyle** — generated from tokens

When tokens change, regenerate all 3.

### 9.5 Override Protocol

User provides partial or full token JSON at setup (Q6):
- Partial: merge with defaults (user values win)
- Full: replace entirely
- Validate: all hex colors valid, all required keys present
- If invalid: show what's wrong, ask to fix or fall back to defaults
- Overriding brand tokens does NOT remove AI Analyst Lab attribution —
  that is controlled separately via `config.attribution.show_attribution`

---

## 10. Non-Negotiable Rules

These rules apply at every layer. Phase 0 agents, Phase 1 builders, and the
final Phase 2 product must all respect them.

1. **No hardcoded data references.** Never embed dataset names, schema prefixes,
   table names, or column names in agent prompts, skills, or helpers. Always
   resolve from the active dataset's manifest and schema files.

2. **Always validate before presenting.** 4-layer validation is not optional.
   Every analysis must pass all 4 layers before findings reach the user.

3. **Always cite the data source.** Every finding must reference which table,
   column, and time range it comes from.

4. **Always flag insufficient data.** If data cannot answer the question, say
   so. Never produce misleading analysis from inadequate data.

5. **Simpson's Paradox check before every conclusion.** Segment-first
   aggregation paradox check is mandatory.

6. **Save outputs to correct locations.** Intermediate: `working/`. Final:
   `outputs/`. Build artifacts: `_build/`.

7. **Offer a path forward.** Never dead-end. When something fails, suggest
   alternatives.

8. **Credentials never in code.** Use secure credential management (secrets
   manager, env vars, key files). Never commit secrets.

9. **CLAUDE.md ≤ 350 lines.** Extract to INDEX files if needed.

10. **Brand tokens are the single source of truth for colors.** CSS, Python
    chart code, and matplotlib styles all read from the same tokens. Zero
    color duplication.

11. **AI Analyst Lab attribution is present by default.** Deck footers,
    chart watermarks, analysis headers, help text, and build artifacts all
    include AI Analyst Lab attribution. Users can disable via
    `config.attribution.show_attribution: false` in genome_config.yaml,
    but it requires an explicit, intentional edit.

12. **No hardcoded database technology.** Never embed specific database
    engines, cache technologies, or connector libraries in agent prompts
    or skills. Always resolve from genome_config.yaml and the data platform
    layer designed by Phase 0.

---

## Appendix A: Existing Connector Adapter Pattern

When the user says "I already have connector scripts" during setup:

1. **connector-inspector** agent reads their Python files
2. Maps their methods to the ConnectionManager interface:
   - `connect()` → establish connection
   - `query(sql)` → execute and return DataFrame
   - `close()` → clean up
3. Generates `helpers/user_connector_adapter.py` — a thin wrapper
4. User's original code stays completely untouched
5. All pipeline agents use the adapter, never the raw connector

## Appendix B: Capability Tier Reduction

**Core tier** (removes from Full):
- Drop agents: root-cause-investigator, opportunity-sizer, experiment-designer
- Drop skills: forecast, patterns, semantic-validation, archive-analysis,
  close-the-loop
- Simplify: validation runs layers 1+3 only (skip 2+4)

**Minimal tier** (removes from Core):
- Drop agents: cohort-analysis, story-architect, narrative-coherence-reviewer,
  storytelling, deck-creator, visual-design-critic
- Drop skills: visualization-patterns, presentation-themes,
  stakeholder-communication, triangulation, guardrails, tracking-gaps
- Keep: data-explorer, source-tieout, descriptive-analytics, overtime-trend,
  validation, chart-maker + data platform skills + question-framing

## Appendix C: Directory Tree (Full Tier)

```
repo/
├── CLAUDE.md                              # Persona file (≤350 lines)
├── genome_config.yaml                     # Setup wizard output
├── data_sources.yaml                      # Connection registry
├── agents/
│   ├── registry.yaml                      # Machine-readable DAG
│   ├── question-framing.md
│   ├── hypothesis.md
│   ├── data-explorer.md
│   ├── source-tieout.md
│   ├── descriptive-analytics.md
│   ├── overtime-trend.md
│   ├── cohort-analysis.md
│   ├── root-cause-investigator.md
│   ├── validation.md
│   ├── opportunity-sizer.md
│   ├── story-architect.md
│   ├── narrative-coherence-reviewer.md
│   ├── chart-maker.md
│   ├── visual-design-critic.md
│   ├── storytelling.md
│   ├── deck-creator.md
│   ├── close-the-loop.md
│   ├── experiment-designer.md
│   └── connector-inspector.md
├── .claude/
│   └── skills/
│       ├── connect-data/skill.md
│       ├── switch-dataset/skill.md
│       ├── datasets/skill.md
│       ├── data-inspect/skill.md
│       ├── data-profiling/skill.md
│       ├── knowledge-bootstrap/skill.md
│       ├── data-quality-check/skill.md
│       ├── cache-sync/skill.md
│       ├── question-framing/skill.md
│       ├── metric-spec/skill.md
│       ├── tracking-gaps/skill.md
│       ├── triangulation/skill.md
│       ├── guardrails/skill.md
│       ├── simpsons-paradox/skill.md
│       ├── analysis-design-spec/skill.md
│       ├── close-the-loop/skill.md
│       ├── run-pipeline/skill.md
│       ├── resume-pipeline/skill.md
│       ├── question-router/skill.md
│       ├── first-run-welcome/skill.md
│       ├── explore/skill.md
│       ├── export/skill.md
│       ├── forecast/skill.md
│       ├── history/skill.md
│       ├── patterns/skill.md
│       ├── semantic-validation/skill.md
│       ├── archive-analysis/skill.md
│       ├── visualization-patterns/skill.md
│       ├── presentation-themes/skill.md
│       └── stakeholder-communication/skill.md
├── helpers/
│   ├── chart_helpers.py
│   ├── sql_helpers.py
│   ├── data_helpers.py
│   ├── stats_helpers.py
│   ├── cache_helpers.py
│   ├── error_helpers.py
│   └── analytics_chart_style.mplstyle
├── templates/
│   ├── deck_skeleton.marp.md
│   └── marp_components.md
├── themes/
│   ├── analytics.css
│   └── analytics-dark.css
├── .knowledge/
│   ├── active.yaml
│   ├── datasets/
│   │   └── {{dataset_name}}/
│   │       ├── manifest.yaml
│   │       ├── schema.md
│   │       ├── quirks.md
│   │       ├── last_profile.md
│   │       ├── access_notes.md
│   │       └── metrics/
│   ├── analyses/
│   ├── user/
│   │   └── profile.yaml
│   └── global/
│       └── frameworks.md
├── data/                                  # Local data (CSV/Parquet fallback)
├── working/                               # Intermediate artifacts
├── outputs/                               # Final deliverables
└── _build/
    ├── BUILD_PLAN.md
    ├── BUILD_STATUS.yaml
    ├── DECISION_LOG.md
    ├── SETUP_CONTEXT.yaml
    ├── integration_review.md
    └── working/                           # Phase 0 debate artifacts
```

---

*End of AI Analyst Genome v1.0 — AI Analyst Lab | aianalystlab.ai*
*Created by Shane Butler, Sravya Madipalli, and Hai Guan.*
